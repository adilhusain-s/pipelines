name: Convert dataset export for batch predict
description: Converts classification dataset export for batch prediction input.
inputs:
- {name: file_paths, type: 'typing.List[str]', description: List of URIs of the files
    storing the batch prediction input.}
- name: classification_type
  type: String
  description: |-
    String representing the problem type: either
    "multiclass" (single-label) or "multilabel".
outputs:
- name: output_dir
  type: JsonArray
  description: |-
    GCS directory where the output files will be stored. Should be
    generated by the pipeline.
- {name: output_files, type: JsonArray}
implementation:
  container:
    image: tensorflow/tensorflow:2.8.0
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def convert_dataset_export_for_batch_predict(
          file_paths,
          classification_type,
          output_dir
      ):
        """Converts classification dataset export for batch prediction input.

        For each processed data item, there will be a JSON object with two fields: a
        text field containing the raw text (either passed in directly or read from
        GCS), and either a single string label or list of string labels, depending on
        the classification type.

        Args:
          file_paths: List of URIs of the files storing the batch prediction input.
          classification_type: String representing the problem type: either
            "multiclass" (single-label) or "multilabel".
          output_dir: GCS directory where the output files will be stored. Should be
            generated by the pipeline.

        Returns:
          Namedtuple of one list under "output_files" key, containing the URIs of the
          JSONL files ready to be consumed by Vertex batch prediction.
        """
        # pylint: disable=g-import-not-at-top
        import collections
        import json
        import os
        import tensorflow as tf
        # pylint: enable=g-import-not-at-top

        # pylint: disable=invalid-name
        MULTILABEL_TYPE = "multilabel"
        TEXT_KEY = "text"
        LABELS_KEY = "labels"
        CLASSIFICATION_ANNOTATION_KEY = "classificationAnnotation"
        CLASSIFICATION_ANNOTATIONS_KEY = "classificationAnnotations"
        DISPLAY_NAME_KEY = "displayName"
        CONTENT_KEY = "textContent"
        GCS_URI_KEY = "textGcsUri"
        # pylint: enable=invalid-name

        output_file_paths = []
        for file_path in file_paths:
          with tf.io.gfile.GFile(file_path) as json_file:
            # Ensure all dirs are present.
            output_file_path = os.path.join(output_dir, os.path.basename(file_path))
            os.makedirs(output_dir, exist_ok=True)

            with tf.io.gfile.GFile(output_file_path, "w") as results_file:
              for dataset_line in json_file:
                json_obj = json.loads(dataset_line)
                result_list = []
                if json_obj.get(CONTENT_KEY):
                  result_list.append(json_obj.get(CONTENT_KEY))
                elif json_obj.get(GCS_URI_KEY):
                  with tf.io.gfile.GFile(json_obj.get(GCS_URI_KEY), "r") as gcs_file:
                    result_list.append(gcs_file.read())
                else:
                  raise ValueError("Text content or GCS URI must be specified.")
                result_obj = {TEXT_KEY: result_list}

                if classification_type == MULTILABEL_TYPE:
                  result_obj[LABELS_KEY] = [
                      annotation[DISPLAY_NAME_KEY]
                      for annotation in json_obj[CLASSIFICATION_ANNOTATIONS_KEY]
                  ]
                else:
                  result_obj[LABELS_KEY] = json_obj[CLASSIFICATION_ANNOTATION_KEY][
                      DISPLAY_NAME_KEY]
                results_file.write(json.dumps(result_obj) + "\n")
            # Subsequent components will not understand "/gcs/" prefix. Convert to use
            # "gs://" prefix for compatibility.
            if output_file_path.startswith("/gcs/"):
              output_file_path = "gs://" + output_file_path[5:]
            output_file_paths.append(output_file_path)
        output_tuple = collections.namedtuple("Outputs", ["output_files"])
        return output_tuple(output_file_paths)

      def _serialize_json(obj) -> str:
          if isinstance(obj, str):
              return obj
          import json

          def default_serializer(obj):
              if hasattr(obj, 'to_struct'):
                  return obj.to_struct()
              else:
                  raise TypeError(
                      "Object of type '%s' is not JSON serializable and does not have .to_struct() method."
                      % obj.__class__.__name__)

          return json.dumps(obj, default=default_serializer, sort_keys=True)

      import json
      import argparse
      _parser = argparse.ArgumentParser(prog='Convert dataset export for batch predict', description='Converts classification dataset export for batch prediction input.')
      _parser.add_argument("--file-paths", dest="file_paths", type=json.loads, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--classification-type", dest="classification_type", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--output-dir", dest="output_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = convert_dataset_export_for_batch_predict(**_parsed_args)

      _output_serializers = [
          _serialize_json,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --file-paths
    - {inputValue: file_paths}
    - --classification-type
    - {inputValue: classification_type}
    - --output-dir
    - {outputPath: output_dir}
    - '----output-paths'
    - {outputPath: output_files}
